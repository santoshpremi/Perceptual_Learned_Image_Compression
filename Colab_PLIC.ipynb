{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNkXJDvf+LFZq/ckMWyQ6qs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/santoshpremi/Perceptual_Learned_Image_Compression/blob/main/Colab_PLIC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Perceptual Learned Image Compression\n",
        "Introduction:\n",
        "With most of the internet traffic being image or (esp.) video data, image compression algorithms\n",
        "are essential to being able to handle this large amount of data. Neural Image Compression (NIC)\n",
        "methods have been shown to achieve significantly better compression rates compared to\n",
        "handcrafted algorithms [1]. However, both approaches can suffer from artifacts or overly\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "smoothed images.\n",
        "Perceptual Image Compression [2, 3] tries to generate reconstructions that are perceived to be of\n",
        "high quality, even if the actual pixel-values might differ from the original image.\n",
        "Goals:\n",
        "Design and implement a NIC method based on a SOTA method\n",
        "Explore different approaches for optimizing for perceptual quality\n",
        "Evaluate on benchmark datasets in terms of distortion, perceptual quality and speed\n",
        "Desirable Experience:\n",
        "Knowledge of Computer Vision, Deep Learning and Compression\n",
        "Experience in the PyTorch or TensorFlow framework\n",
        "\n",
        "References\n",
        "1. He, Dailan, Ziming Yang, Hongjiu Yu, Tongda Xu, Jixiang Luo, Yuan Chen, Chenjian Gao, Xinjie Shi,\n",
        "Hongwei Qin, and Yan Wang. “PO-ELIC: Perception-Oriented Efficient Learned Image Coding.” In\n",
        "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 1763–\n",
        "68. New Orleans, LA, USA: IEEE, 2022. https://doi.org/10.1109/CVPRW56347.2022.00187.\n",
        "2. Mentzer, Fabian, George Toderici, Michael Tschannen, and Eirikur Agustsson. “High-Fidelity\n",
        "Generative Image Compression.” arXiv, October 23, 2020.\n",
        "https://doi.org/10.48550/arXiv.2006.09965.\n",
        "3. Ning, Peirong, Wei Jiang, and Ronggang Wang. “HFLIC: Human Friendly Perceptual Learned Image\n",
        "Compression with Reinforced Transform.” arXiv, May 18, 2023.\n",
        "https://doi.org/10.48550/arXiv.2305.07519.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GklrUDusnsZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HFLIC: Human Friendly Perceptual Learned Image Compression with Reinforced Transform\n",
        "\n",
        "This notebook implements **HFLIC (Human Friendly Perceptual Learned Image Compression)** based on the paper \"HFLIC: Human Friendly Perceptual Learned Image Compression with Reinforced Transform\" by extending ELIC with reinforced transform blocks for better perceptual quality."
      ],
      "metadata": {
        "id": "IhZNJlrcBIOz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmOr1kOEAz-K",
        "outputId": "b1b65adf-6281-44aa-a3e9-edfaaf7a3519",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: compressai in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from compressai) (0.8.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from compressai) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from compressai) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from compressai) (2.2.2)\n",
            "Requirement already satisfied: pybind11>=2.6.0 in /usr/local/lib/python3.12/dist-packages (from compressai) (3.0.1)\n",
            "Requirement already satisfied: pytorch-msssim in /usr/local/lib/python3.12/dist-packages (from compressai) (1.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from compressai) (1.16.3)\n",
            "Requirement already satisfied: setuptools>=68 in /usr/local/lib/python3.12/dist-packages (from compressai) (75.2.0)\n",
            "Requirement already satisfied: tomli>=2.2.1 in /usr/local/lib/python3.12/dist-packages (from compressai) (2.3.0)\n",
            "Requirement already satisfied: torch-geometric>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from compressai) (2.7.0)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.12/dist-packages (from compressai) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from compressai) (0.23.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from compressai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from compressai) (4.15.0)\n",
            "Requirement already satisfied: wheel>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from compressai) (0.45.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->compressai) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->compressai) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->compressai) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->compressai) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->compressai) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->compressai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->compressai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->compressai) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->compressai) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->compressai) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->compressai) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->compressai) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->compressai) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->compressai) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->compressai) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->compressai) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->compressai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->compressai) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->compressai) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.1->compressai) (3.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric>=2.3.0->compressai) (3.13.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric>=2.3.0->compressai) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric>=2.3.0->compressai) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric>=2.3.0->compressai) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric>=2.3.0->compressai) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->compressai) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->compressai) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->compressai) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->compressai) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->compressai) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->compressai) (11.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->compressai) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->compressai) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->compressai) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->compressai) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.1->compressai) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric>=2.3.0->compressai) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.1->compressai) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric>=2.3.0->compressai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric>=2.3.0->compressai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric>=2.3.0->compressai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric>=2.3.0->compressai) (2025.10.5)\n",
            "Requirement already satisfied: lpips in /usr/local/lib/python3.12/dist-packages (0.1.4)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from lpips) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (0.23.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.12/dist-packages (from lpips) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (1.16.3)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.12/dist-packages (from lpips) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=0.4.0->lpips) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.2.1->lpips) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=0.4.0->lpips) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=0.4.0->lpips) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import io\n",
        "import torch\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "!pip install compressai\n",
        "!pip install lpips\n",
        "\n",
        "# Add project path for imports\n",
        "sys.path.append('/content')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f137a4cb",
        "outputId": "e5e74eb3-b9d7-4207-ec4e-1c48063c39a6"
      },
      "source": [
        "import os\n",
        "import inspect\n",
        "import compressai\n",
        "\n",
        "# Get the path to the compressai package\n",
        "compressai_path = os.path.dirname(inspect.getfile(compressai))\n",
        "print(f\"CompressAI package path: {compressai_path}\")\n",
        "\n",
        "# List contents of the compressai directory\n",
        "print(\"Contents of compressai package:\")\n",
        "for item in os.listdir(compressai_path):\n",
        "    print(f\"- {item}\")\n",
        "\n",
        "# Check specifically for the 'metrics' submodule\n",
        "metrics_path = os.path.join(compressai_path, 'metrics')\n",
        "print(f\"\\nChecking for compressai/metrics at: {metrics_path}\")\n",
        "if os.path.exists(metrics_path) and os.path.isdir(metrics_path):\n",
        "    print(\"compressai/metrics directory exists.\")\n",
        "    print(\"Contents of compressai/metrics:\")\n",
        "    for item in os.listdir(metrics_path):\n",
        "        print(f\"  - {item}\")\n",
        "    # Try a direct import again to see if it works after explicit path check\n",
        "    try:\n",
        "        from compressai import metrics\n",
        "        print(\"Successfully imported compressai.metrics.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to import compressai.metrics even after path check: {e}\")\n",
        "else:\n",
        "    print(\"compressai/metrics directory does NOT exist.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CompressAI package path: /usr/local/lib/python3.12/dist-packages/compressai\n",
            "Contents of compressai package:\n",
            "- typing\n",
            "- __init__.py\n",
            "- datasets\n",
            "- layers\n",
            "- ans.cpython-312-x86_64-linux-gnu.so\n",
            "- sadl_codec\n",
            "- registry\n",
            "- latent_codecs\n",
            "- _CXX.cpython-312-x86_64-linux-gnu.so\n",
            "- entropy_models\n",
            "- losses\n",
            "- transforms\n",
            "- zoo\n",
            "- optimizers\n",
            "- __pycache__\n",
            "- version.py\n",
            "- utils\n",
            "- models\n",
            "- ops\n",
            "\n",
            "Checking for compressai/metrics at: /usr/local/lib/python3.12/dist-packages/compressai/metrics\n",
            "compressai/metrics directory does NOT exist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5FSd_N7AbG7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train HFLIC: Vimeo90K dataset and save weights**\n",
        "\n",
        "Training HFLIC model on Vimeo90K dataset with perceptual loss:\n",
        "- **Loss = 0.1·MSE + 0.9·LPIPS + 10.0·λ·BPP**\n",
        "- Model: HFLIC with N=192, M=320\n",
        "- Training on Vimeo90K test split (for fine-tuning)\n",
        "- Saving checkpoints for evaluation\n"
      ],
      "metadata": {
        "id": "v5JWn7zWG92A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "!ls \"/content\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwrByfXKHL4K",
        "outputId": "a79c8bbc-3f02-4693-9773-2731a8dd34d6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write HFLIC model and layer files to disk\n",
        "import os\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs('/content/modules', exist_ok=True)\n",
        "os.makedirs('/content/models', exist_ok=True)\n",
        "\n",
        "# Write layers.py\n",
        "layers_code = '''\"\"\"\n",
        "Layer modules for HFLIC/ELIC architecture\n",
        "Based on ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class GDN(nn.Module):\n",
        "    \"\"\"Generalized Divisive Normalization\"\"\"\n",
        "    def __init__(self, in_channels, inverse=False, beta_min=1e-6, gamma_init=0.1):\n",
        "        super(GDN, self).__init__()\n",
        "        self.inverse = inverse\n",
        "        self.beta_min = beta_min\n",
        "        beta = torch.ones(in_channels)\n",
        "        gamma = torch.eye(in_channels) * gamma_init\n",
        "        self.register_buffer(\"beta\", beta)\n",
        "        self.register_parameter(\"gamma\", nn.Parameter(gamma))\n",
        "\n",
        "    def forward(self, x):\n",
        "        gamma = self.gamma + self.beta_min\n",
        "        beta = self.beta + self.beta_min\n",
        "\n",
        "        if self.inverse:\n",
        "            norm = torch.matmul(gamma, x.permute(0, 2, 3, 1).unsqueeze(-1))\n",
        "            norm = norm.squeeze(-1).permute(0, 3, 1, 2)\n",
        "            return x * torch.sqrt(norm)\n",
        "        else:\n",
        "            norm = torch.matmul(gamma, (x ** 2).permute(0, 2, 3, 1).unsqueeze(-1))\n",
        "            norm = norm.squeeze(-1).permute(0, 3, 1, 2)\n",
        "            return x / torch.sqrt(norm + beta.unsqueeze(0).unsqueeze(2).unsqueeze(3))\n",
        "\n",
        "\n",
        "class IGDN(nn.Module):\n",
        "    \"\"\"Inverse Generalized Divisive Normalization\"\"\"\n",
        "    def __init__(self, in_channels, beta_min=1e-6, gamma_init=0.1):\n",
        "        super(IGDN, self).__init__()\n",
        "        self.gdn = GDN(in_channels, inverse=True, beta_min=beta_min, gamma_init=gamma_init)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.gdn(x)\n",
        "\n",
        "\n",
        "class ResidualBottleneck(nn.Module):\n",
        "    \"\"\"Residual bottleneck block for ELIC/HFLIC\"\"\"\n",
        "    def __init__(self, N, M, act=nn.ReLU(inplace=True)):\n",
        "        super(ResidualBottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(N, M, 1, stride=1, padding=0)\n",
        "        self.conv2 = nn.Conv2d(M, M, 3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(M, N, 1, stride=1, padding=0)\n",
        "        self.act = act\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.act(self.conv1(x))\n",
        "        out = self.act(self.conv2(out))\n",
        "        out = self.conv3(out)\n",
        "        return out + identity\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    \"\"\"Attention mechanism for reinforced transform in HFLIC\"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.channels = channels\n",
        "        self.conv = nn.Conv2d(channels, channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attention = torch.sigmoid(self.conv(x))\n",
        "        return x * attention\n",
        "\n",
        "\n",
        "class ReinforcedTransform(nn.Module):\n",
        "    \"\"\"Reinforced Transform module for HFLIC - no downsampling, just attention + transformation\"\"\"\n",
        "    def __init__(self, N, M):\n",
        "        super(ReinforcedTransform, self).__init__()\n",
        "        self.attention = AttentionBlock(N)\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(N, M, 3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(M, N, 3, stride=1, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_attn = self.attention(x)\n",
        "        x_transformed = self.conv(x_attn)\n",
        "        return x + x_transformed  # Residual connection\n",
        "\n",
        "\n",
        "class ContextualAttention(nn.Module):\n",
        "    \"\"\"Contextual attention module for adaptive coding\"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super(ContextualAttention, self).__init__()\n",
        "        self.conv_query = nn.Conv2d(channels, channels // 4, 1)\n",
        "        self.conv_key = nn.Conv2d(channels, channels // 4, 1)\n",
        "        self.conv_value = nn.Conv2d(channels, channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.size()\n",
        "        query = self.conv_query(x).view(B, -1, H * W)\n",
        "        key = self.conv_key(x).view(B, -1, H * W)\n",
        "        value = self.conv_value(x).view(B, -1, H * W)\n",
        "\n",
        "        attention = torch.softmax(torch.bmm(query.transpose(1, 2), key), dim=-1)\n",
        "        out = torch.bmm(value, attention.transpose(1, 2))\n",
        "        return out.view(B, C, H, W)\n",
        "'''\n",
        "\n",
        "with open('/content/modules/layers.py', 'w') as f:\n",
        "    f.write(layers_code)\n",
        "\n",
        "# Write elic.py\n",
        "elic_code = '''\"\"\"\n",
        "ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding\n",
        "Base implementation for HFLIC\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from compressai.entropy_models import EntropyBottleneck, GaussianConditional\n",
        "from compressai.layers import GDN\n",
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.append(str(Path(__file__).parent.parent))\n",
        "from modules.layers import ResidualBottleneck, ContextualAttention\n",
        "\n",
        "\n",
        "class ELICAnalysis(nn.Module):\n",
        "    \"\"\"Analysis transform for ELIC\"\"\"\n",
        "    def __init__(self, N=192, M=320):\n",
        "        super(ELICAnalysis, self).__init__()\n",
        "        self.N = N\n",
        "        self.M = M\n",
        "\n",
        "        # Initial convolution\n",
        "        self.conv1 = nn.Conv2d(3, N, 5, stride=2, padding=2)\n",
        "        self.gdn1 = GDN(N)\n",
        "\n",
        "        # Residual blocks with attention\n",
        "        self.res1 = ResidualBottleneck(N, N * 2)\n",
        "        self.res2 = ResidualBottleneck(N, N * 2)\n",
        "        self.attn1 = ContextualAttention(N)\n",
        "\n",
        "        # Downsampling\n",
        "        self.conv2 = nn.Conv2d(N, N, 5, stride=2, padding=2)\n",
        "        self.gdn2 = GDN(N)\n",
        "\n",
        "        self.res3 = ResidualBottleneck(N, N * 2)\n",
        "        self.res4 = ResidualBottleneck(N, N * 2)\n",
        "        self.attn2 = ContextualAttention(N)\n",
        "\n",
        "        # Final transform\n",
        "        self.conv3 = nn.Conv2d(N, M, 3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.gdn1(self.conv1(x))\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        x = self.attn1(x)\n",
        "        x = self.gdn2(self.conv2(x))\n",
        "        x = self.res3(x)\n",
        "        x = self.res4(x)\n",
        "        x = self.attn2(x)\n",
        "        x = self.conv3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ELICSynthesis(nn.Module):\n",
        "    \"\"\"Synthesis transform for ELIC\"\"\"\n",
        "    def __init__(self, N=192, M=320):\n",
        "        super(ELICSynthesis, self).__init__()\n",
        "        self.N = N\n",
        "        self.M = M\n",
        "\n",
        "        # Initial transform\n",
        "        self.conv1 = nn.Conv2d(M, N, 3, stride=1, padding=1)\n",
        "\n",
        "        # Residual blocks with attention\n",
        "        self.attn1 = ContextualAttention(N)\n",
        "        self.res1 = ResidualBottleneck(N, N * 2)\n",
        "        self.res2 = ResidualBottleneck(N, N * 2)\n",
        "\n",
        "        # Upsampling\n",
        "        self.conv2 = nn.ConvTranspose2d(N, N, 5, stride=2, padding=2, output_padding=1)\n",
        "        self.igdn1 = GDN(N, inverse=True)\n",
        "\n",
        "        self.attn2 = ContextualAttention(N)\n",
        "        self.res3 = ResidualBottleneck(N, N * 2)\n",
        "        self.res4 = ResidualBottleneck(N, N * 2)\n",
        "\n",
        "        # Final transform\n",
        "        self.conv3 = nn.ConvTranspose2d(N, 3, 5, stride=2, padding=2, output_padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.attn1(x)\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        x = self.igdn1(self.conv2(x))\n",
        "        x = self.attn2(x)\n",
        "        x = self.res3(x)\n",
        "        x = self.res4(x)\n",
        "        x = self.conv3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ELIC(nn.Module):\n",
        "    \"\"\"\n",
        "    ELIC: Efficient Learned Image Compression\n",
        "    Base model for HFLIC\n",
        "    \"\"\"\n",
        "    def __init__(self, N=192, M=320):\n",
        "        super(ELIC, self).__init__()\n",
        "        self.N = N\n",
        "        self.M = M\n",
        "\n",
        "        self.analysis = ELICAnalysis(N, M)\n",
        "        self.synthesis = ELICSynthesis(N, M)\n",
        "\n",
        "        # Entropy models\n",
        "        self.hyperprior_entropy_bottleneck = EntropyBottleneck(M) # Entropy bottleneck for hyperprior (z)\n",
        "        self.gaussian_conditional = GaussianConditional(None)\n",
        "\n",
        "        # Hyper-synthesis network to predict scales and means for y from z_hat\n",
        "        self.h_s = nn.Sequential(\n",
        "            nn.Conv2d(M, M * 2, 3, stride=1, padding=1), # Outputs 2*M channels for scales and means\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(M * 2, M * 2, 3, stride=1, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Analysis transform\n",
        "        y = self.analysis(x)\n",
        "\n",
        "        # Context model for hyperprior (z from abs(y) as in original code)\n",
        "        z = torch.abs(y)\n",
        "        z_hat, z_likelihoods = self.hyperprior_entropy_bottleneck(z)\n",
        "\n",
        "        # Predict scales and means for y from z_hat using hyper-synthesis\n",
        "        gaussian_params = self.h_s(z_hat)\n",
        "        scales_hat, means_hat = gaussian_params.chunk(2, 1) # Split into M channels for scales, M for means\n",
        "\n",
        "        # Gaussian conditional for y\n",
        "        y_hat, y_likelihoods = self.gaussian_conditional(y, scales_hat, means=means_hat)\n",
        "\n",
        "        # Synthesis transform\n",
        "        x_hat = self.synthesis(y_hat)\n",
        "\n",
        "        return {\n",
        "            \"x_hat\": x_hat.clamp(0, 1),\n",
        "            \"likelihoods\": {\"y\": y_likelihoods, \"z\": z_likelihoods},\n",
        "        }\n",
        "'''\n",
        "\n",
        "with open('/content/models/elic.py', 'w') as f:\n",
        "    f.write(elic_code)\n",
        "\n",
        "# Write hflic.py\n",
        "hflic_code = '''\"\"\"\n",
        "HFLIC: Human Friendly Perceptual Learned Image Compression with Reinforced Transform\n",
        "Extends ELIC with reinforced transform for better perceptual quality\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import sys\n",
        "from pathlib import Path\n",
        "sys.path.append(str(Path(__file__).parent.parent))\n",
        "from models.elic import ELIC\n",
        "from modules.layers import ReinforcedTransform, AttentionBlock\n",
        "\n",
        "\n",
        "class HFLICAnalysis(nn.Module):\n",
        "    \"\"\"Analysis transform for HFLIC with reinforced transform\"\"\"\n",
        "    def __init__(self, N=192, M=320):\n",
        "        super(HFLICAnalysis, self).__init__()\n",
        "        from compressai.layers import GDN\n",
        "        from modules.layers import ResidualBottleneck, ContextualAttention\n",
        "\n",
        "        self.N = N\n",
        "        self.M = M\n",
        "\n",
        "        # Initial convolution\n",
        "        self.conv1 = nn.Conv2d(3, N, 5, stride=2, padding=2)\n",
        "        self.gdn1 = GDN(N)\n",
        "\n",
        "        # Reinforced transform blocks\n",
        "        self.reinforced1 = ReinforcedTransform(N, N)\n",
        "        self.reinforced2 = ReinforcedTransform(N, N)\n",
        "\n",
        "        # Residual blocks with attention\n",
        "        self.res1 = ResidualBottleneck(N, N * 2)\n",
        "        self.res2 = ResidualBottleneck(N, N * 2)\n",
        "        self.attn1 = ContextualAttention(N)\n",
        "\n",
        "        # Downsampling with reinforced transform\n",
        "        self.conv2 = nn.Conv2d(N, N, 5, stride=2, padding=2)\n",
        "        self.gdn2 = GDN(N)\n",
        "\n",
        "        self.reinforced3 = ReinforcedTransform(N, N)\n",
        "        self.res3 = ResidualBottleneck(N, N * 2)\n",
        "        self.res4 = ResidualBottleneck(N, N * 2)\n",
        "        self.attn2 = ContextualAttention(N)\n",
        "\n",
        "        # Final transform\n",
        "        self.conv3 = nn.Conv2d(N, M, 3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.gdn1(self.conv1(x))\n",
        "        x = self.reinforced1(x)\n",
        "        x = self.reinforced2(x)\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        x = self.attn1(x)\n",
        "        x = self.gdn2(self.conv2(x))\n",
        "        x = self.reinforced3(x)\n",
        "        x = self.res3(x)\n",
        "        x = self.res4(x)\n",
        "        x = self.attn2(x)\n",
        "        x = self.conv3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class HFLICSynthesis(nn.Module):\n",
        "    \"\"\"Synthesis transform for HFLIC with reinforced transform\"\"\"\n",
        "    def __init__(self, N=192, M=320):\n",
        "        super(HFLICSynthesis, self).__init__()\n",
        "        from compressai.layers import GDN\n",
        "        from modules.layers import ResidualBottleneck, ContextualAttention\n",
        "\n",
        "        self.N = N\n",
        "        self.M = M\n",
        "\n",
        "        # Initial transform\n",
        "        self.conv1 = nn.Conv2d(M, N, 3, stride=1, padding=1)\n",
        "\n",
        "        # Residual blocks with attention\n",
        "        self.attn1 = ContextualAttention(N)\n",
        "        self.res1 = ResidualBottleneck(N, N * 2)\n",
        "        self.res2 = ResidualBottleneck(N, N * 2)\n",
        "\n",
        "        # Upsampling with reinforced transform\n",
        "        self.conv2 = nn.ConvTranspose2d(N, N, 5, stride=2, padding=2, output_padding=1)\n",
        "        self.igdn1 = GDN(N, inverse=True)\n",
        "\n",
        "        self.reinforced1 = ReinforcedTransform(N, N)\n",
        "        self.attn2 = ContextualAttention(N)\n",
        "        self.res3 = ResidualBottleneck(N, N * 2)\n",
        "        self.res4 = ResidualBottleneck(N, N * 2)\n",
        "\n",
        "        # Final transform\n",
        "        self.conv3 = nn.ConvTranspose2d(N, 3, 5, stride=2, padding=2, output_padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.attn1(x)\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        x = self.igdn1(self.conv2(x))\n",
        "        x = self.reinforced1(x)\n",
        "        x = self.attn2(x)\n",
        "        x = self.res3(x)\n",
        "        x = self.res4(x)\n",
        "        x = self.conv3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class HFLIC(nn.Module):\n",
        "    \"\"\"\n",
        "    HFLIC: Human Friendly Perceptual Learned Image Compression with Reinforced Transform\n",
        "\n",
        "    Extends ELIC by incorporating reinforced transform blocks that enhance\n",
        "    perceptual quality through attention mechanisms.\n",
        "    \"\"\"\n",
        "    def __init__(self, N=192, M=320):\n",
        "        super(HFLIC, self).__init__()\n",
        "        self.N = N\n",
        "        self.M = M\n",
        "\n",
        "        self.analysis = HFLICAnalysis(N, M)\n",
        "        self.synthesis = HFLICSynthesis(N, M)\n",
        "\n",
        "        # Entropy models\n",
        "        from compressai.entropy_models import EntropyBottleneck, GaussianConditional\n",
        "        self.hyperprior_entropy_bottleneck = EntropyBottleneck(M) # Entropy bottleneck for hyperprior (z)\n",
        "        self.gaussian_conditional = GaussianConditional(None)\n",
        "\n",
        "        # Hyper-synthesis network to predict scales and means for y from z_hat\n",
        "        self.h_s = nn.Sequential(\n",
        "            nn.Conv2d(M, M * 2, 3, stride=1, padding=1), # Outputs 2*M channels for scales and means\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(M * 2, M * 2, 3, stride=1, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Analysis transform\n",
        "        y = self.analysis(x)\n",
        "\n",
        "        # Context model for hyperprior (z from abs(y) as in original code)\n",
        "        z = torch.abs(y)\n",
        "        z_hat, z_likelihoods = self.hyperprior_entropy_bottleneck(z)\n",
        "\n",
        "        # Predict scales and means for y from z_hat using hyper-synthesis\n",
        "        gaussian_params = self.h_s(z_hat)\n",
        "        scales_hat, means_hat = gaussian_params.chunk(2, 1) # Split into M channels for scales, M for means\n",
        "\n",
        "        # Gaussian conditional for y\n",
        "        y_hat, y_likelihoods = self.gaussian_conditional(y, scales_hat, means=means_hat)\n",
        "\n",
        "        # Synthesis transform\n",
        "        x_hat = self.synthesis(y_hat)\n",
        "\n",
        "        return {\n",
        "            \"x_hat\": x_hat.clamp(0, 1),\n",
        "            \"likelihoods\": {\"y\": y_likelihoods, \"z\": z_likelihoods},\n",
        "        }\n",
        "\n",
        "    def compress(self, x):\n",
        "        \"\"\"Compress image\"\"\"\n",
        "        y = self.analysis(x)\n",
        "        # For compression, we need the actual quantized latent z_hat for hyperprior\n",
        "        z = torch.abs(y)\n",
        "        z_hat = self.hyperprior_entropy_bottleneck.quantize(z)\n",
        "\n",
        "        # Predict scales and means for y from z_hat\n",
        "        gaussian_params = self.h_s(z_hat)\n",
        "        scales_hat, means_hat = gaussian_params.chunk(2, 1)\n",
        "\n",
        "        # Quantize y using gaussian conditional\n",
        "        y_hat = self.gaussian_conditional.quantize(y, scales_hat, means=means_hat)\n",
        "\n",
        "        return y_hat\n",
        "\n",
        "    def decompress(self, y_string, z_string, shape):\n",
        "        \"\"\"Decompress image\"\"\"\n",
        "        # Decompression logic would go here\n",
        "        # This part requires a full implementation of decoding from bitstreams\n",
        "        # and is more complex than a simple forward pass.\n",
        "        # For now, it's a placeholder.\n",
        "        raise NotImplementedError(\"Decompression is not fully implemented yet.\")\n",
        "'''\n",
        "\n",
        "with open('/content/models/hflic.py', 'w') as f:\n",
        "    f.write(hflic_code)\n",
        "\n",
        "print(\"HFLIC model files written successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIn24HWBI0Mx",
        "outputId": "51ed764f-dd3e-49e1-e259-26a6c20ef211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HFLIC model files written successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from models.hflic import HFLIC\n",
        "from datasets.vimeo90k import Vimeo90KSingleFrameDataset\n",
        "from loss.perceptual_loss import PerceptualLoss\n",
        "from utils.utils import AverageMeter, compute_psnr, create_train_transform\n",
        "\n",
        "\n",
        "def parse_args(argv):\n",
        "    parser = argparse.ArgumentParser(description=\"HFLIC training loop inside notebook\")\n",
        "    parser.add_argument('--dataset', type=str, required=True, help='Path to Vimeo90K dataset')\n",
        "    parser.add_argument('--epochs', type=int, default=100)\n",
        "    parser.add_argument('--batch-size', type=int, default=8)\n",
        "    parser.add_argument('--learning-rate', type=float, default=1e-4)\n",
        "    parser.add_argument('--aux-learning-rate', type=float, default=1e-3)\n",
        "    parser.add_argument('--num-workers', type=int, default=4)\n",
        "    parser.add_argument('--patch-size', type=int, nargs=2, default=(256, 256))\n",
        "    parser.add_argument('--lambda', dest='lmbda', type=float, default=0.08)\n",
        "    parser.add_argument('--clip-max-norm', type=float, default=0.1)\n",
        "    parser.add_argument('--cuda', action='store_true')\n",
        "    parser.add_argument('--print-freq', type=int, default=100)\n",
        "    return parser.parse_args(argv)\n",
        "\n",
        "\n",
        "def configure_optimizers(net, args):\n",
        "    params, aux_params = [], []\n",
        "    for name, param in net.named_parameters():\n",
        "        if 'entropy_bottleneck' in name or 'gaussian_conditional' in name:\n",
        "            aux_params.append(param)\n",
        "        else:\n",
        "            params.append(param)\n",
        "\n",
        "    main_optim = optim.Adam(params, lr=args.learning_rate)\n",
        "    aux_optim = optim.Adam(aux_params, lr=args.aux_learning_rate) if aux_params else None\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "        main_optim,\n",
        "        step_size=max(args.epochs // 4, 1),\n",
        "        gamma=0.5,\n",
        "    )\n",
        "    scheduler_aux = (\n",
        "        torch.optim.lr_scheduler.StepLR(aux_optim, step_size=max(args.epochs // 4, 1), gamma=0.5)\n",
        "        if aux_optim is not None\n",
        "        else None\n",
        "    )\n",
        "\n",
        "    return main_optim, aux_optim, scheduler, scheduler_aux\n",
        "\n",
        "\n",
        "def train_one_epoch(model, criterion, dataloader, optimizer, aux_optimizer, epoch, args):\n",
        "    model.train()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    loss_meter = AverageMeter()\n",
        "    bpp_meter = AverageMeter()\n",
        "    psnr_meter = AverageMeter()\n",
        "    lpips_meter = AverageMeter()\n",
        "\n",
        "    for i, (images, _) in enumerate(dataloader):\n",
        "        images = images.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        if aux_optimizer is not None:\n",
        "            aux_optimizer.zero_grad()\n",
        "\n",
        "        output = model(images)\n",
        "        loss_dict = criterion(output, images)\n",
        "        total_loss = loss_dict['loss']\n",
        "        total_loss.backward()\n",
        "\n",
        "        if args.clip_max_norm > 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_max_norm)\n",
        "        optimizer.step()\n",
        "        if aux_optimizer is not None:\n",
        "            aux_optimizer.step()\n",
        "\n",
        "        batch_size = images.size(0)\n",
        "        loss_meter.update(total_loss.item(), batch_size)\n",
        "        bpp_meter.update(loss_dict['bpp_loss'].item(), batch_size)\n",
        "        lpips_meter.update(loss_dict['lpips_loss'].item(), batch_size)\n",
        "        psnr_meter.update(compute_psnr(images, output['x_hat']), batch_size)\n",
        "\n",
        "        if i % args.print_freq == 0:\n",
        "            print(\n",
        "                f\"Epoch {epoch} | Iter {i}/{len(dataloader)} | \"\n",
        "                f\"Loss {loss_meter.avg:.4f} | BPP {bpp_meter.avg:.4f} | \"\n",
        "                f\"LPIPS {lpips_meter.avg:.4f} | PSNR {psnr_meter.avg:.2f}\"\n",
        "            )\n",
        "\n",
        "    return {\n",
        "        'loss': loss_meter.avg,\n",
        "        'bpp': bpp_meter.avg,\n",
        "        'psnr': psnr_meter.avg,\n",
        "        'lpips': lpips_meter.avg,\n",
        "        'mse': mse_meter.avg,\n",
        "    }\n",
        "\n",
        "\n",
        "def main(argv):\n",
        "    args = parse_args(argv)\n",
        "    device = 'cuda' if args.cuda and torch.cuda.is_available() else 'cpu'\n",
        "    print(f'Using device: {device}')\n",
        "\n",
        "    crop = args.patch_size[0] if isinstance(args.patch_size, (list, tuple)) else args.patch_size\n",
        "    transform = create_train_transform(crop_size=crop)\n",
        "    dataset = Vimeo90KSingleFrameDataset(\n",
        "        root_dir=args.dataset,\n",
        "        split='train',\n",
        "        transform=transform\n",
        "    )\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=args.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=args.num_workers,\n",
        "        pin_memory=device == 'cuda'\n",
        "    )\n",
        "\n",
        "    model = HFLIC().to(device)\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        model = nn.DataParallel(model)\n",
        "\n",
        "    criterion = PerceptualLoss(lmbda=args.lmbda).to(device)\n",
        "    optimizer, aux_optimizer, scheduler, scheduler_aux = configure_optimizers(model, args)\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        metrics = train_one_epoch(\n",
        "            model,\n",
        "            criterion,\n",
        "            dataloader,\n",
        "            optimizer,\n",
        "            aux_optimizer,\n",
        "            epoch,\n",
        "            clip_max_norm=args.clip_max_norm,\n",
        "            print_freq=args.print_freq,\n",
        "        )\n",
        "        print(\n",
        "            \"Epoch {}/{} summary: Loss {:.4f} | BPP {:.4f} | LPIPS {:.4f} | PSNR {:.2f} | MSE {:.6f}\".format(\n",
        "                epoch + 1,\n",
        "                args.epochs,\n",
        "                metrics['loss'],\n",
        "                metrics['bpp'],\n",
        "                metrics['lpips'],\n",
        "                metrics['psnr'],\n",
        "                metrics['mse'],\n",
        "            )\n",
        "        )\n",
        "\n",
        "        scheduler.step()\n",
        "        if scheduler_aux is not None:\n",
        "            scheduler_aux.step()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main([\n",
        "        '--dataset', '/content/drive/My Drive/vimeo_test_clean',\n",
        "        '--epochs', '100',\n",
        "        '--cuda',\n",
        "        '--batch-size', '4',\n",
        "        '--learning-rate', '1e-4',\n",
        "        '--aux-learning-rate', '1e-3',\n",
        "        '--clip-max-norm', '0.1',\n",
        "        '--print-freq', '50',\n",
        "    ])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "pzgGzRfhHPKw",
        "outputId": "79b572af-f5a4-48d6-9828-fb1fd899ed9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'models'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3094744392.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhflic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHFLIC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvimeo90k\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVimeo90KSingleFrameDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperceptual_loss\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPerceptualLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluating HFLIC: Kodak dataset with saved weights**\n",
        "\n",
        "Evaluating trained HFLIC model on Kodak dataset (24 standard test images) with comprehensive metrics:\n",
        "- **PSNR**: Peak Signal-to-Noise Ratio (dB)\n",
        "- **LPIPS**: Learned Perceptual Image Patch Similarity (lower is better)\n",
        "- **BPP**: Bits Per Pixel (compression efficiency)\n",
        "- **MSE**: Mean Squared Error\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "jdBUjNuUp3RC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import torch\n",
        "import lpips\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from compressai.zoo import image_models\n",
        "from compressai.losses import RateDistortionLoss\n",
        "\n",
        "# Define a custom dataset class for Kodak images\n",
        "class KodakDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_files = [f for f in os.listdir(root_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.image_files[idx])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, 0  # Return 0 for compatibility with dataloader (label is not used)\n",
        "\n",
        "# Utility class to compute running average of the losses\n",
        "class AverageMeter:\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "# Function to compute PSNR\n",
        "def compute_psnr(a, b, max_val: float = 1.0) -> float:\n",
        "    mse = torch.mean((a - b) ** 2).item()\n",
        "    if mse == 0:\n",
        "        return float('inf')\n",
        "    psnr = -10 * np.log10(mse)\n",
        "    return psnr\n",
        "\n",
        "# Function to compute BPP\n",
        "def compute_bpp(out_net, num_pixels):\n",
        "    \"\"\"Compute bits per pixel from likelihoods\"\"\"\n",
        "    bpp = 0.0\n",
        "    for likelihoods in out_net[\"likelihoods\"].values():\n",
        "        bpp += torch.log(likelihoods + 1e-10).sum() / (-math.log(2) * num_pixels)\n",
        "    return bpp.item()\n",
        "\n",
        "# Function to evaluate the model on Kodak dataset with PSNR, LPIPS, and BPP\n",
        "def evaluate_model(model, dataloader, device, model_name=\"Model\"):\n",
        "    model.eval()\n",
        "    psnr_meter = AverageMeter()\n",
        "    bpp_meter = AverageMeter()\n",
        "    lpips_meter = AverageMeter()\n",
        "\n",
        "    lpips_loss = lpips.LPIPS(net='alex').to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, _ in dataloader:\n",
        "            images = images.to(device)\n",
        "            out_net = model(images)\n",
        "\n",
        "            # Compute PSNR\n",
        "            psnr = compute_psnr(images, out_net['x_hat'])\n",
        "            psnr_meter.update(psnr, images.size(0))\n",
        "\n",
        "            # Compute BPP (corrected: include batch size)\n",
        "            num_pixels = images.size(0) * images.size(2) * images.size(3)\n",
        "            bpp = compute_bpp(out_net, num_pixels)\n",
        "            bpp_meter.update(bpp)\n",
        "\n",
        "            # Compute LPIPS\n",
        "            # LPIPS expects values in range [-1, 1]\n",
        "            images_lpips = images * 2.0 - 1.0\n",
        "            x_hat_lpips = out_net['x_hat'] * 2.0 - 1.0\n",
        "            lpips_value = lpips_loss(images_lpips, x_hat_lpips).mean().item()\n",
        "            lpips_meter.update(lpips_value, images.size(0))\n",
        "\n",
        "    print(f\"\\n{model_name} Results:\")\n",
        "    print(f\"  PSNR: {psnr_meter.avg:.3f} dB\")\n",
        "    print(f\"  BPP:  {bpp_meter.avg:.4f}\")\n",
        "    print(f\"  LPIPS: {lpips_meter.avg:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'PSNR': psnr_meter.avg,\n",
        "        'BPP': bpp_meter.avg,\n",
        "        'LPIPS': lpips_meter.avg\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    # Configuration\n",
        "    checkpoint_path = '/content/checkpoint_best_loss.pth.tar'  # Path to the best checkpoint\n",
        "    kodak_dataset_path = '/content/drive/My Drive/kodak'  # Path to Kodak dataset\n",
        "    patch_size = (256, 256)\n",
        "    batch_size = 8\n",
        "    quality = 3\n",
        "    N = 192  # HFLIC channel parameters\n",
        "    M = 320\n",
        "\n",
        "    # Setup device\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load Kodak Dataset\n",
        "    test_transforms = transforms.Compose([\n",
        "        transforms.Resize(patch_size),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    kodak_dataset = KodakDataset(root_dir=kodak_dataset_path, transform=test_transforms)\n",
        "    kodak_dataloader = DataLoader(\n",
        "        kodak_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"COMPREHENSIVE EVALUATION ON KODAK DATASET\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # ============================================\n",
        "    # 1. Evaluate Pretrained Baseline Model\n",
        "    # ============================================\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"1. Evaluating PRETRAINED Baseline Model\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    from compressai.zoo import image_models\n",
        "    pretrained_model = image_models[\"mbt2018-mean\"](quality=quality, pretrained=True).to(device)\n",
        "    pretrained_results = evaluate_model(\n",
        "        pretrained_model,\n",
        "        kodak_dataloader,\n",
        "        device,\n",
        "        model_name=\"Pretrained Baseline\"\n",
        "    )\n",
        "\n",
        "    # ============================================\n",
        "    # 2. Evaluate Fine-tuned Model\n",
        "    # ============================================\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"2. Evaluating FINE-TUNED Model (LPIPS)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        # Initialize HFLIC model exactly as in training\n",
        "        from models.hflic import HFLIC\n",
        "        fine_tuned_model = HFLIC(N=N, M=M).to(device)\n",
        "\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "        # Handle DataParallel if needed\n",
        "        state_dict = checkpoint['state_dict']\n",
        "        if any(k.startswith('module.') for k in state_dict.keys()):\n",
        "            from collections import OrderedDict\n",
        "            new_state_dict = OrderedDict()\n",
        "            for k, v in state_dict.items():\n",
        "                name = k[7:] if k.startswith('module.') else k\n",
        "                new_state_dict[name] = v\n",
        "            state_dict = new_state_dict\n",
        "\n",
        "        # Filter out size-0 parameters (lazy-initialized) - they'll be initialized on first forward\n",
        "        filtered_state_dict = {}\n",
        "        model_state_dict = fine_tuned_model.state_dict()\n",
        "        for k, v in state_dict.items():\n",
        "            if k in model_state_dict:\n",
        "                if v.shape == model_state_dict[k].shape:\n",
        "                    filtered_state_dict[k] = v\n",
        "                elif v.numel() == 0:\n",
        "                    # Skip lazy-initialized parameters (they'll be initialized on first forward)\n",
        "                    print(f\"  Skipping lazy-initialized parameter: {k}\")\n",
        "                else:\n",
        "                    # Size mismatch - try to load anyway or skip\n",
        "                    print(f\"  Warning: Size mismatch for {k}: checkpoint {v.shape} vs model {model_state_dict[k].shape}\")\n",
        "\n",
        "        fine_tuned_model.load_state_dict(filtered_state_dict, strict=False)\n",
        "        fine_tuned_model.eval()\n",
        "\n",
        "        fine_tuned_results = evaluate_model(\n",
        "            fine_tuned_model,\n",
        "            kodak_dataloader,\n",
        "            device,\n",
        "            model_name=\"Fine-tuned (LPIPS)\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError loading fine-tuned model: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        fine_tuned_results = {'PSNR': 0.0, 'BPP': 0.0, 'LPIPS': 0.0}\n",
        "\n",
        "    # ============================================\n",
        "    # 3. Comparison Table\n",
        "    # ============================================\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"3. COMPARISON TABLE\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    comparison_data = {\n",
        "        'Model': ['Pretrained Baseline', 'Fine-tuned (LPIPS)'],\n",
        "        'PSNR (dB)': [f\"{pretrained_results['PSNR']:.3f}\", f\"{fine_tuned_results['PSNR']:.3f}\"],\n",
        "        'BPP': [f\"{pretrained_results['BPP']:.4f}\", f\"{fine_tuned_results['BPP']:.4f}\"],\n",
        "        'LPIPS': [f\"{pretrained_results['LPIPS']:.4f}\", f\"{fine_tuned_results['LPIPS']:.4f}\"]\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(comparison_data)\n",
        "    print(\"\\n\" + df.to_string(index=False))\n",
        "\n",
        "    # Calculate improvements\n",
        "    if fine_tuned_results['PSNR'] > 0:\n",
        "        print(\"\\n\" + \"-\" * 60)\n",
        "        print(\"IMPROVEMENTS (Fine-tuned vs Pretrained):\")\n",
        "        print(\"-\" * 60)\n",
        "        psnr_diff = fine_tuned_results['PSNR'] - pretrained_results['PSNR']\n",
        "        bpp_diff = fine_tuned_results['BPP'] - pretrained_results['BPP']\n",
        "        lpips_diff = fine_tuned_results['LPIPS'] - pretrained_results['LPIPS']\n",
        "\n",
        "        print(f\"  PSNR:  {psnr_diff:+.3f} dB ({'↑ Improved' if psnr_diff > 0 else '↓ Worse'})\")\n",
        "        print(f\"  BPP:   {bpp_diff:+.4f} ({'↓ Improved' if bpp_diff < 0 else '↑ Worse'})\")\n",
        "        print(f\"  LPIPS: {lpips_diff:+.4f} ({'↓ Improved' if lpips_diff < 0 else '↑ Worse'})\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Evaluation Complete!\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AutwOC_1p21O",
        "outputId": "77dcf6eb-c380-43b1-eeee-3bf8293001d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "============================================================\n",
            "COMPREHENSIVE EVALUATION ON KODAK DATASET\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "1. Evaluating PRETRAINED Baseline Model\n",
            "============================================================\n",
            "Downloading: \"https://compressai.s3.amazonaws.com/models/v1/mbt2018-mean-3-723404a8.pth.tar\" to /root/.cache/torch/hub/checkpoints/mbt2018-mean-3-723404a8.pth.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27.6M/27.6M [00:02<00:00, 12.4MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233M/233M [00:01<00:00, 244MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.12/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "\n",
            "Pretrained Baseline Results:\n",
            "  PSNR: 30.708 dB\n",
            "  BPP:  0.3036\n",
            "  LPIPS: 0.2062\n",
            "\n",
            "============================================================\n",
            "2. Evaluating FINE-TUNED Model (LPIPS)\n",
            "============================================================\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.12/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "\n",
            "Fine-tuned (LPIPS) Results:\n",
            "  PSNR: 6.554 dB\n",
            "  BPP:  102.0340\n",
            "  LPIPS: 1.0055\n",
            "\n",
            "============================================================\n",
            "3. COMPARISON TABLE\n",
            "============================================================\n",
            "\n",
            "              Model PSNR (dB)      BPP  LPIPS\n",
            "Pretrained Baseline    30.708   0.3036 0.2062\n",
            " Fine-tuned (LPIPS)     6.554 102.0340 1.0055\n",
            "\n",
            "------------------------------------------------------------\n",
            "IMPROVEMENTS (Fine-tuned vs Pretrained):\n",
            "------------------------------------------------------------\n",
            "  PSNR:  -24.154 dB (↓ Worse)\n",
            "  BPP:   +101.7305 (↑ Worse)\n",
            "  LPIPS: +0.7993 (↑ Worse)\n",
            "\n",
            "============================================================\n",
            "Evaluation Complete!\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}